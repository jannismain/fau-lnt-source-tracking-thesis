\section{Implementation Details}
\label{chap:implementation}

At the beginning of this chapter, the topic of computational complexity is addressed. After that, the simulation framework is laid out and the procedure that has been followed for the evaluation trials is described. In the end, certain evaluation scenarios are established to analyse the possibilities and limitations of this implementation. This will assist in drawing conclusions about the strengths and shortcomings of the algorithm in this particular setup and may allow for more general findings to be deducted.

\subsection{Computational Complexity}\label{sec:computationalComplexity} As computational complexity, and therefore execution time, is one of the measures to compare different algorithms and their implementations, the hardware on which these evaluations have been executed, needs to be mentioned. All trials and profiling benchmarks have been done on a Retina Macbook Pro 13" (MGX92LL/A), that is equipped with a 2.8 GHz Core i5-4308U CPU and 16GB of DDR3L-1600Mhz RAM. Although there were no explicit efforts made to parallelise the implementation, \matlab R2017a utilises both available CPU cores of the i5 for certain operations. During the implementation and initial trial phase it became obvious that computational complexity is an issue that has to be addressed on most hardware configurations to successfully execute the proposed algorithms. First, execution speed had been seen as the main bottleneck, which is why the implementation has been vectorised where possible. However, this vectorisation came with the drawback of higher memory requirements, which proofed to be an issue on conventional hardware, especially for the replication trials.


While trying to replicate the results from \cite{Schwartz2014} for the static case, the 16GB RAM did not suffice the memory requirements of the fully vectorized implementation of the algorithm. Therefore, these trials have been carried out on different hardware with 32GB RAM, which was utilized to almost full capacity. This is due to the fact, that the experiments in \cite{Schwartz2014} estimated the model parameters for all grid points and not only those shown in white in \autoref{fig:setup}, which increased $|\pall|$ from $41 \cdot 41 = 1681$ to $61\cdot61=3721$, more than double in size. They also choose a wider frequency band (64 frequency bins instead of 24). Lastly, in their implementation the Gaussian component weights $\psips$ are calculated seperately for each source, doubling the size of the intermediary results for each \gls{em} yet again. The size of the largest temporary results in matlab with these parameters is around $30$GB.
