\section{Acoustic Source Localisation}
\label{sec:algLocEst}
%\begin{itemize}
%\item mehr Zwischenschritte
%\item Erklärung der Ergebnisse, Annahmen, roter Faden der Herleitung... Was machen wir und warum?
%\item Was ist die Intuition dahinter? Was bedeuten die Größen? Was ist die interessierende Größe? Was sind Hilfsgrößen?
%\end{itemize}

If the likelihood function of the observed data \eqref{eq:mle} were to be maximised directly, the following equation would have to be solved
\begin{equation}
    \ln f(\bm\phi;\bm\psi,\bm\sigma^2) = \ln\prod_{t,k}\sum_{s,\bm p}\psi_{\bm p}\prod_m\mathcal{N}^c\big(\phi_m(t,k);\tilde\phi^k_m(\bm p),\sigma^2_{s}\big).
\end{equation}
Because the sum over all possible grid points $\bm p$ prevents the logarithm from acting directly on the exponential term, solving this function results in a complicated expression, which should be avoided. This is where a so-called \textit{latent variable} is introduced. Here, the latent variable is defined as an indicator $z$, that a certain time-frequency bin $(t,k)$ belongs to an active source at position $\bm p$ 
\begin{equation}
	\z=\begin{cases}
	    1& \text{source $s$ at $\bm p$ active in $(t,k)$-bin},\\
       0& \text{otherwise}.
	\end{cases}
\end{equation}

The expectation of the hidden variable is equal to the probability of a source to exist at position $\bm p$ and to have shown activity in time-frequency bin $(t, k)$ 
\begin{equation}
    E\{\z\}=\psips.
\end{equation}

As sparsity of the source signals is assumed, meaning there is only one source active in each time-frequency bin, the latent variable is a 1-of-$|\mathcal{P}|$ representation, where $|\mathcal{P}|$ denotes the cardinality of the set of all grid points, i.e. there is one $z(t',k',s',\bm p')=1$ whereas $z(t',k',s',\bm p)=0\ \forall\ \bm p\in\mathcal{P}\smallsetminus\{\bm p'\}$. The property that makes indicators in combination with \glspl{gmm} so useful is their ability to simplify the sum over all components into a product
\begin{equation}
\label{eq:indicator-sum-product}
    \sum_{i}x_i\cdot z_i=\prod_i (x_i)^{z_i}.
\end{equation}

With $\bm z=\text{vec}_{t,k,s,\bm p}(\z)$ being the concatenated vector of the latent variable $z$ across all time-frequency-bins, sources and positions, the probability density of $\bm z$ can be written as \begin{equation}
    p(\bm z;\bm\theta)=\prod_{t,k}\sum_{s,\bm p}\psips\cdot\z,
\end{equation}

%TODO: Besser herausarbeiten, das mean = PRP reading
where $\bm\theta=[\bm\psi,\bm\sigma^2]$ is the set of target parameters.

%Note, that $\mu$ is not part of this set, as the Gaussian components in this model are evenly spaced throughout the room and each component mean corresponds to one grid point. This deviates from the way the \gls{em} algorithm is commonly used together with a \gls{gmm}, where, in addition to the Gaussian component weights $\bm\psi$, generally both mean and covariance are part of the estimation.

As was described in Section \ref{sec:em} and further argued above, due to the complexity involved when maximising the log-likelihood function of the observations $\ln p(\bm\phi;\bm\psi, \bm\sigma^2)$ directly, a tight lower bound at $\bm\theta^{(l-1)}$ is derived and maximised instead. This lower-bound will utilize the latent variable to reduce complexity of the optimisation problem and is given in \eqref{eq:L} . From \eqref{eq:L} one can see, that to evaluate this function, the complete-data likelihood $p(\bm\phi,\bm z; \bm\theta)$ as well as the conditional likelihood $p(\bm z\given{\bm\phi, \bm\theta^{(l-1)}})$ is required. Given the hidden data, the conditional likelihood of the observations is given by:
\begin{equation}
    p(\bm\phi |\bm z;\bm\theta)=\prod_{t,k}\sum_{s,\bm p}\prod_{m}\gauss\cdot\z.
\end{equation}

Multiplying the densities of the hidden variable and observations yields the joint probability distribution of the complete data:
\begin{equation}
    p(\bm\phi,\bm z;\bm\theta)=p(\bm z;\bm\theta)\cdot p(\bm\phi |\bm z;\bm\theta)=\prod_{t,k}\sum_{s,\bm p}\psips\cdot\z\prod_{m}\gauss .
\end{equation}

The log-likelihood of the complete data is given by
\begin{align}
    \ln f(\bm\phi,\bm z; \bm\theta) &= \ln \left [ \lcompl \right ].\\
    \intertext{First, the logarithm can be moved inside the product}
    \ln f(\bm\phi,\bm z; \bm\theta) &=\sum_{t,k}\ln \left [\sum_{s,\bm p}\psips\cdot\z\prod_{m}\gauss\right ].\\
    \intertext{Now, the property of the indicator stated in \eqref{eq:indicator-sum-product} will be used to simplify the sum, that divides the logarithm and the exponential term, to a product}
    \ln f(\bm\phi,\bm z; \bm\theta) &=\sum_{t,k}\ln \left [\prod_{s,\bm p}\Big (\psips\prod_{m}\gauss\Big )^{\z}\right ].\\
    \intertext{Applying basic algebraic rules for the logarithm yields}
    \label{eq:complete-data}
%    &=\sum_{t,k}\z\cdot\ln \left [\prod_{\bm p}\Big (\psips\prod_{m}\gauss\Big )\right ]\\
%    \intertext{Finally, using $\ln(x\cdot y)=\ln(x)+\ln(y)$ one more time}
    \ln f(\bm\phi,\bm z; \bm\theta) &=\sum_{t,k,s,\bm p}\z\cdot\ln \left [\psips\prod_{m}\gauss\right ].
\end{align}

\subsubsection*{E-Step}
Inserting \eqref{eq:complete-data} into \eqref{eq:e-step} yields the complete $\mathcal{Q}$-function
\begin{align}
    \Q &=E\left\{\ln(f(\bm\phi,\bm z;\bm\theta))\mid \bm\phi;\bm\theta^{(l-1)}\right\}=\\
       &=\sum_{t,k,s,\bm p}E_{\bm z}\left\{\z\mid\phi(t,k);\bm\theta^{(l-1)}\right\}\cdot\left [ \ln\psips+\sum_{m}\ln\gauss\right]\nonumber\\
       %&= \sum_{t,k,s,\bm p}\mulast\cdot\left [\ln\psips+\sum_{m}\ln\gauss\right ].
\end{align}

The expected value of $\bm z$ given the observation $\bm\phi$ is given by
\begin{align}
\label{eq:responsibility}
    E_{\bm z}\left\{\z\mid\phi(t,k);\bm\theta^{(l-1)}\right\}&=\frac{\pdffunc}{\sum_{s,\bm p}\pdffunc}\\                  
                                                  &\triangleq\mulast .
\end{align}

$h^{(l)}(t,k,s,\bm p)$ can also be interpreted as the responsibility that the Gaussian component at position $\bm p$ takes for explaining the observation $\bm\phi(t,k)$ in time-frequency-bin $(t,k)$ \cite[p.~432]{Bishop2006}.

\subsubsection*{M-Step}
As seen in \eqref{eq:m-step}, the M-Step consists of maximizing the $\mathcal{Q}$-function with regard to the elements of the parameter set $\bm\theta$. This is done by computing the gradient of the $\mathcal{Q}$-function
\begin{equation}
\label{eq:m_step_theta_derivative}
    \bm\theta^{(l)}=\nabla_{\bm\theta}\Q,
\end{equation}

which means computing the partial derivatives of the target parameters $\psi_{s\bm p}^{(l)}$ and $\sigma_s^{2,(l)}$
\begin{align}
    \label{eq:m_step_sigma_derivative}
    \sigma_{s}^{2, (l)}&=\frac{\partial}{\partial \sigma^2}\Q,\\
    \label{eq:m_step_psi_derivative}
    \psi_{s\bm p}^{(l)}&=\frac{\partial}{\partial \psips}\Q.
\end{align}

As $\psi_{s\bm p}$ is constrained by $\sum_{\bm p}\psi_{s\bm p}=1$, to fulfill the requirements of a probability over grid positions, constrained optimisation using a Lagrange mulitplier has to be applied:
\begin{equation}
    g(\lambda,\psips)=\Q+\lambda\left (\sum_{\bm p}\psi_{s\bm p}-1\right ).
\end{equation}

Derivation and setting to zero gives
\begin{align}
    \frac{\partial\Q}{\partial\psips}+\lambda &=0,\\
    \frac{\sum_{t,k}\mulast}{\psips}+\lambda &=0,\\
    -\frac{\sum_{t,k}\mulast}{\lambda}&=\psips
\label{eq:psi-lambda}.
\end{align}

We compute the sum over $\bm p$ on both sides to solve for $\lambda$
\begin{align}
    \sum_{s,\bm p}\psips &=-\frac{1}{\lambda}\sum_{t,k,s,\bm p}\mulast,\\
    S&=-\frac{1}{\lambda}\sum_{t,k}S,\\
    \lambda &= -TK.\label{eq:lambda}
\end{align}

Finally, we insert \eqref{eq:lambda} into \eqref{eq:psi-lambda} to get
\begin{equation}\label{eq:m_step_psi}
    \psi_{s\bm p}^{(l)}=\frac{\sum_{t,k}\mulast}{T\cdot K}
\end{equation}

Calculating the partial derivate of $\sigma^2$ results in

\begin{equation}\label{eq:m_step_var}
    \sigma_s^{2,(l)}=\frac{\sum_{t,k,s,\bm p}\mulast\cdot\left | \phi_m(t,k)-\tilde\phi_m^k(\bm p)\right |^2}{M\cdot \sum_{t,k,s,\bm p}\mulast}.
\end{equation}

This concludes the M-Step. When all iterations have been carried out, either because convergence was determined or a fixed number of iterations has been reached, the estimated Gaussian weights are summed up for all $S$

\begin{equation}
    \hat\psi_{\bm p} = \frac{1}{S}\sum_{s}\psips.
\end{equation}

in order to find the actual estimated source locations by selecting the positions $\hat p_s$ associated with the $S$ largest values of $\hat{\bm\psi} = \text{vec}_{\bm p}(\hat\psi_{\bm p})$

\begin{equation}\label{eq:find_positions}
    \hat{\bm p}_s=\arglmax_{\bm p}\hat{\psi}_{\bm p}\ \forall\ s
\end{equation}

where lmax yields a different local maxima for each $s$.

\input{data/algorithms/alg-loc}

\paragraph{Deviation from \cite{Schwartz2014}}
In the algorithm described above, the Gaussian weights $\psips$ are calculated for each source seperately. In their experiments, the authors of \cite{Schwartz2014} then use a different initialisation for each $\psips^{\text{(0)}}\ \forall\ s$, so that the resulting parameter estimates $\hat\psi_{s\bm p}$ differ from each other. Our experiments will show that using the same initialisation for all $\psips^{\text{(0)}}$ will results in identical estimates. Using different initial parameters will yield different estimates, but prior knowledge about the source positions is necessary to gain an advantage in doing so. Therefore it is assumed that removing the $S$-dimension from all parameter estimates will yield equal localisation performance while reducing the complexity of the algorithm by a factor of $S$. As the results in \autoref{sec:results-localisation} will show, this is especially useful for trials with $S>2$, as memory requirements of the temporary results of the algorithm will exceed commonly employed hardware otherwise.

In addition to the benefits of reduced complexity and memory requirements, this is also a step towards achieving independence from the number of sources. In \cite{Schwartz2014}, the number of sources is assumed to be known a priori. With an implementation that estimates a joint $\psip$ for all sources, this prior knowledge is only required in the last step of the algorithm, when the $S$ largest values of $\psip$ are used to identify the source position estimates. By introducing a threshold value $\psi_{\text{threshold}}$ and selecting all positions, where $\psip>\psi_{\text{threshold}}$, the constraint of a-priori knowledge about the number of sources is eliminated. However, this threshold depends on the distribution of $\hat{\bm\psi}$, which is influenced by the environmental parameters. The experiment results will show, if a threshold value could, in theory, be applied to generalise the source localisation algorithm to an unknown number of sources $S$.
